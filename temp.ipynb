{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnWek502gvA1LC5h7+SteR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yunyoungwoo/2024S-Ajou-ML-FP/blob/main/temp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Colab 환경에 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3NB2EuKy3LW",
        "outputId": "3ec18042-d305-4afa-bf11-8eebe6231d14"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# DataFrame을 만들기 위한 빈 리스트 생성\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# training 및 validation 디렉토리 경로\n",
        "training_directory = '/content/drive/MyDrive/Colab Notebooks/dataset/training'\n",
        "validation_directory = '/content/drive/MyDrive/Colab Notebooks/dataset/training'\n",
        "\n",
        "# 클래스 이름 리스트 생성\n",
        "class_names = os.listdir(training_directory)\n",
        "\n",
        "# 필요한 정보만 추출하여 리스트에 추가\n",
        "for class_name in class_names:\n",
        "    # training 데이터 처리\n",
        "    training_image_directory = os.path.join(training_directory, class_name)\n",
        "    training_label_file = os.path.join(training_image_directory, f'label{class_name}.json')\n",
        "\n",
        "    # JSON 파일 불러오기\n",
        "    with open(training_label_file, 'r') as f:\n",
        "        training_data = json.load(f)\n",
        "\n",
        "    # JSON 파일에서 필요한 정보 추출하여 리스트에 추가\n",
        "    for image_data in training_data:\n",
        "        image_id = image_data['image_id']\n",
        "        image_file_name = image_data['image_file_name']\n",
        "        value_6 = image_data['value_6']\n",
        "\n",
        "        # 이미지 경로\n",
        "        image_path = os.path.join(training_image_directory, f'image{image_id}.jpg')\n",
        "\n",
        "        image_paths.append(image_path)\n",
        "        labels.append(value_6)\n",
        "\n",
        "# DataFrame 생성\n",
        "df = pd.DataFrame({'image_path': image_paths, 'value_6': labels})\n",
        "\n",
        "# DataFrame 확인\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "fSOgF7W6vt0-",
        "outputId": "7fedd2be-cc2a-4a0a-b099-8fc1241a5d1f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/path_to_json_file.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3ae52fe08709>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# JSON 파일 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/path_to_json_file.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# 원-핫 인코딩\n",
        "encoder = OneHotEncoder()\n",
        "labels_one_hot = encoder.fit_transform(df['label'].values.reshape(-1, 1)).toarray()\n",
        "\n",
        "# DataFrame에 원-핫 인코딩된 라벨 추가\n",
        "df_encoded = df.copy()\n",
        "df_encoded[['label_0', 'label_1', 'label_2', 'label_3']] = labels_one_hot\n",
        "\n",
        "# 데이터 증강 설정\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "# 이미지 경로와 라벨 가져오기\n",
        "image_paths = df_encoded['image_path'].tolist()\n",
        "labels = df_encoded[['label_0', 'label_1', 'label_2', 'label_3']].values\n",
        "\n",
        "# 데이터 증강 적용 후 이미지와 라벨 생성\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "\n",
        "for image_path, label in zip(image_paths, labels):\n",
        "    image = plt.imread(image_path)\n",
        "    image = image.reshape((1,) + image.shape)  # 모델에 맞는 shape로 변환\n",
        "    label = label.reshape((1,) + label.shape)  # 모델에 맞는 shape로 변환\n",
        "\n",
        "    # 데이터 증강 수행\n",
        "    for x_batch, y_batch in datagen.flow(image, label, batch_size=1):\n",
        "        augmented_images.append(x_batch[0])\n",
        "        augmented_labels.append(y_batch[0])\n",
        "        break  # 한 번 증강된 데이터만 사용\n",
        "\n",
        "# DataFrame에 증강된 이미지 경로와 라벨 추가\n",
        "df_augmented = pd.DataFrame({'image_path': augmented_images,\n",
        "                             'label_0': [label[0] for label in augmented_labels],\n",
        "                             'label_1': [label[1] for label in augmented_labels],\n",
        "                             'label_2': [label[2] for label in augmented_labels],\n",
        "                             'label_3': [label[3] for label in augmented_labels]})\n",
        "\n",
        "# 증강된 DataFrame 확인\n",
        "print(df_augmented.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "bJ0WVjv6OrtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mhj76fPesfME"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "\n",
        "seed = 1\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "OgUiIA0Ms_4m"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "wuFhcvIHtLer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "0fs2Of82tNUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/kaggle/input/2022-ai-tp-18011886/train\"\n",
        "test_path = \"/kaggle/input/2022-ai-tp-18011886/test\"\n",
        "all_label = os.listdir(train_path)"
      ],
      "metadata": {
        "id": "sn6z3Rs0tOCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class train_dataset():\n",
        "    def __init__(self):\n",
        "        self.data_path = []\n",
        "        self.label_idx = []\n",
        "\n",
        "        for idx, label in enumerate(all_label):\n",
        "            for data_path in tqdm(os.listdir(os.path.join(train_path,label))):\n",
        "                path = os.path.join(label,data_path)\n",
        "                self.data_path.append(path)\n",
        "                self.label_idx.append(idx)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((320,320)),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "        path = self.data_path[idx]\n",
        "        img = Image.open(os.path.join(train_path,path))\n",
        "        img = img.convert('RGB')\n",
        "        img = transform(img)\n",
        "        return img, self.label_idx[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_path)\n",
        "\n",
        "class test_dataset():\n",
        "    def __init__(self):\n",
        "        self.data_path = []\n",
        "\n",
        "        for data_path in tqdm(os.listdir(os.path.join(test_path))):\n",
        "            path = data_path\n",
        "            self.data_path.append(path)\n",
        "\n",
        "        self.data_path.sort()\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((320,320)),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "        path = self.data_path[idx]\n",
        "        img = Image.open(os.path.join(test_path,path))\n",
        "        img = img.convert('RGB')\n",
        "        img = transform(img)\n",
        "        return img, 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_path)"
      ],
      "metadata": {
        "id": "-wXiE4Y5tRid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_dataset = train_dataset()\n",
        "test_dataset = test_dataset()\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                         shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                         shuffle=False)"
      ],
      "metadata": {
        "id": "tIxc5kBOtU_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = torch.nn.Linear(2048, 4, bias=True)\n",
        "torch.nn.init.xavier_normal_(model.fc.weight)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "dqD2JC1StXE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0005)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
      ],
      "metadata": {
        "id": "wOLRsLGatbTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for x,y in tqdm(train_loader):\n",
        "        #import pdb;pdb.set_trace()\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x).to(device)\n",
        "        cost = loss(out, y).to(device)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        avg_cost +=cost.item()/batch_size\n",
        "\n",
        "    print(epoch+1,avg_cost)"
      ],
      "metadata": {
        "id": "e521RbYJtioF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    corr_pred = list()\n",
        "    test = list()\n",
        "    for x,_ in test_loader:\n",
        "        x = x.to(device)\n",
        "        out = model(x).to(device)\n",
        "        pred = torch.argmax(out,dim=1).cpu().detach().numpy()\n",
        "        corr_pred.extend(pred)\n",
        "\n",
        "    for i in range(len(corr_pred)):\n",
        "        test.append(all_label[corr_pred[i]])"
      ],
      "metadata": {
        "id": "kd_KrQ5rtkao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('/kaggle/input/2022-ai-tp-18011886/sample_submit.csv',encoding='cp949')\n",
        "submit['label'] = test\n",
        "submit.to_csv('submit.csv',index=False,encoding='cp949')"
      ],
      "metadata": {
        "id": "Nts2zYfptl8D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}