{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa3+Tdo0zj+TV89mQJRjMe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yunyoungwoo/2024S-Ajou-ML-FP/blob/main/CNN_balanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "2dwNwO1upk83"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as v2\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nIMLDMkh_B0_",
        "outputId": "560f44cf-c8f1-434d-df50-08501f03ab61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터프레임을 사용하여 커스텀 데이터셋 생성\n",
        "class BaseDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe, transform):\n",
        "        self.df = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.df.iloc[index]['image_path']  # 이미지 파일 경로 컬럼 이름이 'image_path'라고 가정\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        transformed_img = self.transform(img)\n",
        "\n",
        "        # 원-핫 인코딩된 클래스 레이블을 가져옴\n",
        "        class_labels = self.df.iloc[index][['class_0', 'class_1', 'class_2', 'class_3']].values.astype(int)\n",
        "        class_id = torch.argmax(torch.tensor(class_labels)).item()  # 정수형 레이블로 변환\n",
        "\n",
        "        return transformed_img, class_id"
      ],
      "metadata": {
        "id": "sG8ucEO4qJGQ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),  # 랜덤 리사이즈 및 자르기\n",
        "    #transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),  # 컬러 지터\n",
        "    transforms.RandomRotation(degrees=90),  # 랜덤 회전\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # 랜덤 수평 뒤집기\n",
        "    transforms.RandomVerticalFlip(p=0.5),  # 랜덤 수직 뒤집\n",
        "    transforms.ToTensor(),  # torch 텐서로 변환\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 정규화\n",
        "]) # 하이퍼파라미터"
      ],
      "metadata": {
        "id": "V9wFHDclqMEK"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 데이터프레임 불러오기\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/FP/augmented1_training.pkl', 'rb') as f:\n",
        "    training_df = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/FP/validation_df.pkl', 'rb') as f:\n",
        "    validation_df = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/FP/test_df.pkl', 'rb') as f:\n",
        "    test_df = pickle.load(f)\n",
        "\n",
        "training_df.head()\n",
        "len(training_df)"
      ],
      "metadata": {
        "id": "r_iz3HcQ_nN5",
        "outputId": "8c7827ca-7e00-4a48-a032-eb1e564e49d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4000"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터프레임을 사용하여 데이터셋 생성\n",
        "train_dataset = BaseDataset(training_df, transform=train_transforms)\n",
        "val_dataset = BaseDataset(validation_df, transform=train_transforms)\n",
        "test_dataset = BaseDataset(test_df, transform=train_transforms)"
      ],
      "metadata": {
        "id": "beUGhrPRqRNJ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터로더 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "oDp7IOpIqTyp"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedCNN(nn.Module):\n",
        "    def __init__(self, num_classes=4):  # 클래스 수를 인자로 받도록 수정\n",
        "        super(ImprovedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(in_features=64 * 14 * 14, out_features=1024)  # Fully Connected Layer 크기 수정\n",
        "        self.fc2 = nn.Linear(in_features=1024, out_features=128)  # Fully Connected Layer 크기 줄이기\n",
        "        self.fc3 = nn.Linear(in_features=128, out_features=num_classes)  # 클래스 수에 맞게 수정\n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(8)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(16)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(32)\n",
        "        self.batchnorm4 = nn.BatchNorm2d(64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.batchnorm1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.batchnorm2(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.batchnorm3(self.conv3(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.batchnorm4(self.conv4(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # 배치 크기에 맞게 변환\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "spapVgxZqcFt"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성 및 CUDA 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ImprovedCNN().to(device)"
      ],
      "metadata": {
        "id": "B8ywIzUgqr4P"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수 및 최적화 함수, 스케쥴러 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.001)  # AdamW 옵티마이저\n",
        "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)  # CosineAnnealingLR 학습률 스케줄러"
      ],
      "metadata": {
        "id": "jziMszFRqs3h"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, model, criterion, optimizer, lr_scheduler):\n",
        "    total_samples = 0\n",
        "    num_batches = len(train_loader)\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    for i, (data, targets) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)  # 이미지 경로 제거\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        _, predictions = outputs.max(1)\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "        epoch_accuracy += torch.sum(predictions == targets).item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    accuracy = epoch_accuracy / total_samples\n",
        "    macro_f1 = f1_score(all_targets, all_predictions, average='macro')\n",
        "    return accuracy, epoch_loss / num_batches, macro_f1"
      ],
      "metadata": {
        "id": "DvbuWQusqZ_W"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(val_loader, model, criterion):\n",
        "    total_samples = 0\n",
        "    num_batches = len(val_loader)\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for i, (data, targets) in enumerate(val_loader):\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)  # 이미지 경로 제거\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            epoch_loss += loss.item()\n",
        "            _, predictions = outputs.max(1)\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            epoch_accuracy += torch.sum(predictions == targets).item()\n",
        "            total_samples += targets.size(0)\n",
        "\n",
        "    accuracy = epoch_accuracy / total_samples\n",
        "    macro_f1 = f1_score(all_targets, all_predictions, average='macro')\n",
        "    return accuracy, epoch_loss / num_batches, macro_f1"
      ],
      "metadata": {
        "id": "t0SREEbrqvcS"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 및 검증 실행\n",
        "EPOCHS = 50 #하이퍼파라미터\n",
        "patience = 5\n",
        "counter = 0\n",
        "best_loss = np.inf\n",
        "logs = {'train_loss': [], 'train_acc': [], 'train_f1': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}"
      ],
      "metadata": {
        "id": "VfmA0nSmsEEk"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    train_acc, train_loss, train_f1 = train(train_loader, model, criterion, optimizer, lr_scheduler)\n",
        "    val_acc, val_loss, val_f1 = test(val_loader, model, criterion)\n",
        "    print(f'epoch: {epoch} \\\n",
        "    train_loss = {train_loss:.4f}, train_acc: {train_acc:.4f}, train_f1: {train_f1:.4f} \\\n",
        "    val_loss = {val_loss:.4f}, val_acc: {val_acc:.4f}, val_f1: {val_f1:.4f} \\\n",
        "    learning rate: {optimizer.param_groups[0][\"lr\"]}')\n",
        "    logs['train_loss'].append(train_loss)\n",
        "    logs['train_acc'].append(train_acc)\n",
        "    logs['train_f1'].append(train_f1)\n",
        "    logs['val_loss'].append(val_loss)\n",
        "    logs['val_acc'].append(val_acc)\n",
        "    logs['val_f1'].append(val_f1)\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        counter = 0\n",
        "        best_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/FP/Improved_CNN_best.pth\")\n",
        "    else:\n",
        "        counter += 1\n",
        "    if counter >= patience:\n",
        "        test_acc, val_loss, test_f1 = test(test_loader, model, criterion)\n",
        "        print(\"Early stop!\")\n",
        "        print(f\"Test accuracy: {test_acc}, Test f1: {test_f1}\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "OulL9zy9qwgC",
        "outputId": "d7e82eb3-7752-413b-d3ef-9b02a3e43788",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0     train_loss = 1.1523, train_acc: 0.4637, train_f1: 0.4520     val_loss = 1.0130, val_acc: 0.5200, val_f1: 0.5301     learning rate: 9.755282581475769e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 1/50 [13:21<10:54:52, 801.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1     train_loss = 1.0574, train_acc: 0.5072, train_f1: 0.4972     val_loss = 0.9633, val_acc: 0.5467, val_f1: 0.5476     learning rate: 9.045084971874737e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 3/50 [20:46<4:36:36, 353.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 2     train_loss = 1.0268, train_acc: 0.5200, train_f1: 0.5114     val_loss = 0.9616, val_acc: 0.5567, val_f1: 0.5537     learning rate: 7.938926261462366e-05\n",
            "epoch: 3     train_loss = 0.9836, train_acc: 0.5525, train_f1: 0.5464     val_loss = 0.9285, val_acc: 0.5533, val_f1: 0.5542     learning rate: 6.545084971874737e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 5/50 [29:07<3:37:22, 289.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 4     train_loss = 0.9738, train_acc: 0.5655, train_f1: 0.5605     val_loss = 0.9299, val_acc: 0.5667, val_f1: 0.5665     learning rate: 4.9999999999999996e-05\n",
            "epoch: 5     train_loss = 0.9514, train_acc: 0.5683, train_f1: 0.5659     val_loss = 0.8977, val_acc: 0.5667, val_f1: 0.5537     learning rate: 3.454915028125263e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 7/50 [37:24<3:11:03, 266.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 6     train_loss = 0.9307, train_acc: 0.5927, train_f1: 0.5907     val_loss = 0.9266, val_acc: 0.5500, val_f1: 0.5502     learning rate: 2.0610737385376345e-05\n",
            "epoch: 7     train_loss = 0.9143, train_acc: 0.5895, train_f1: 0.5873     val_loss = 0.8951, val_acc: 0.5700, val_f1: 0.5704     learning rate: 9.549150281252631e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 9/50 [45:38<2:55:13, 256.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 8     train_loss = 0.9184, train_acc: 0.5900, train_f1: 0.5878     val_loss = 0.9070, val_acc: 0.5467, val_f1: 0.5450     learning rate: 2.447174185242323e-06\n",
            "epoch: 9     train_loss = 0.9136, train_acc: 0.5940, train_f1: 0.5933     val_loss = 0.8919, val_acc: 0.5700, val_f1: 0.5723     learning rate: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 11/50 [53:33<2:39:43, 245.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10     train_loss = 0.8972, train_acc: 0.5998, train_f1: 0.5980     val_loss = 0.9028, val_acc: 0.5833, val_f1: 0.5765     learning rate: 2.4471741852423237e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 12/50 [57:44<2:36:31, 247.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 11     train_loss = 0.9077, train_acc: 0.5913, train_f1: 0.5888     val_loss = 0.8958, val_acc: 0.5600, val_f1: 0.5561     learning rate: 9.549150281252667e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 13/50 [1:01:53<2:32:46, 247.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 12     train_loss = 0.8992, train_acc: 0.5972, train_f1: 0.5954     val_loss = 0.9007, val_acc: 0.5733, val_f1: 0.5697     learning rate: 2.0610737385376434e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 14/50 [1:06:05<2:29:30, 249.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 13     train_loss = 0.9227, train_acc: 0.5900, train_f1: 0.5881     val_loss = 0.8902, val_acc: 0.5733, val_f1: 0.5692     learning rate: 3.4549150281252785e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 15/50 [1:10:14<2:25:19, 249.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 14     train_loss = 0.9080, train_acc: 0.6035, train_f1: 0.6003     val_loss = 0.8940, val_acc: 0.5433, val_f1: 0.5376     learning rate: 5.0000000000000226e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 16/50 [1:14:23<2:21:09, 249.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 15     train_loss = 0.9212, train_acc: 0.5835, train_f1: 0.5797     val_loss = 0.8883, val_acc: 0.5833, val_f1: 0.5728     learning rate: 6.545084971874767e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 17/50 [1:18:31<2:16:49, 248.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 16     train_loss = 0.9258, train_acc: 0.5807, train_f1: 0.5760     val_loss = 0.8693, val_acc: 0.5833, val_f1: 0.5794     learning rate: 7.938926261462401e-05\n",
            "epoch: 17     train_loss = 0.9114, train_acc: 0.5917, train_f1: 0.5882     val_loss = 0.8542, val_acc: 0.6067, val_f1: 0.6025     learning rate: 9.045084971874779e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 19/50 [1:26:47<2:08:16, 248.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 18     train_loss = 0.9205, train_acc: 0.5877, train_f1: 0.5845     val_loss = 0.8953, val_acc: 0.5767, val_f1: 0.5725     learning rate: 9.755282581475812e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 20/50 [1:30:57<2:04:23, 248.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 19     train_loss = 0.9202, train_acc: 0.5897, train_f1: 0.5875     val_loss = 0.8981, val_acc: 0.5733, val_f1: 0.5449     learning rate: 0.00010000000000000044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 21/50 [1:35:09<2:00:37, 249.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 20     train_loss = 0.9089, train_acc: 0.5938, train_f1: 0.5916     val_loss = 0.8582, val_acc: 0.6100, val_f1: 0.6070     learning rate: 9.75528258147581e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 22/50 [1:39:18<1:56:24, 249.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 21     train_loss = 0.9033, train_acc: 0.6022, train_f1: 0.5982     val_loss = 0.9708, val_acc: 0.5333, val_f1: 0.5176     learning rate: 9.04508497187478e-05\n",
            "epoch: 22     train_loss = 0.8938, train_acc: 0.6028, train_f1: 0.6010     val_loss = 0.8730, val_acc: 0.5733, val_f1: 0.5541     learning rate: 7.938926261462401e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 22/50 [1:43:36<2:11:52, 282.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stop!\n",
            "Test accuracy: 0.58, Test f1: 0.5557373781885472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9xgCjINXnB6Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}